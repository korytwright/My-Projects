Chapter 1

Terms:

Cluster- one or many pods

Pod-is a group of applications providing the same service (replicas)

Node-is a physical device that house the pod(s)

Velocity-is measured not in terms of the raw number of features you can ship per hour or day, but rather in terms of the number of things you can ship while maintaining a highly available service.

Scaling (of both software and teams)

Abstracting your infrastructure

Efficiency

Decoupling-making sure front-end and back-end are completely seperated; front end would use API to comminucate with backend.

***********************************************************************************************************************

Chapter 2 Creating and Running Containers

Docker consist of images and containers.
	- And image can either be created by a Dockerfile, or is pulled down from a repo
	- Docker containers are created from Docker images (the evironment and variable declared inside of the Dockerfile)

A container can either be a system container, or an application container.
	- System containers seek to mimic virtual machines and often run a full boot process. They often include a set of system services typically found in a VM, such as ssh, cron, and syslog
	- Application containers commonly run a single application

Container images are constructed of a series of filesystem layers, where each layer inherits and modifies the layer that came before it.

Container images are typically combined with a container configuration file, which provides instructions on how to set up the container environment and execute anapplication entrypoint

The container root filesystem and configuration file are typically bundled using the Docker image format.

Fork - Is when you build off of a container image and it is independant of the parenet image (starting its own branch).



***********************************************************************************************************************

Chapter 3 Installing Kubes on AWS, Google Cloud Platform, Microsoft Azure and Raspberry Pi


$ kubectl get nodes (list all nodes in the cluster)

$ kubectl describe nodes node-1 (see basic information about a specific node)

Cluster components:
Kube Proxy-is responsible for routing network traffic to load-balancedservices in the Kubernetes cluster (ran using Kubernetes DaemonSet).

Kube DNS-which provides naming and discovery for the services that are defined in the cluster (ran using Kubernetes deployment).

Kuberenets UI is ran using deployment; get information about UI dashboard: $ kubectl get deployments --namespace=kube-system kubernetes-dashboard

Everything contained in Kubernetes is represented by a RESTful resource (API).

***********************************************************************************************************************

Chapter 4 Common kubectl Commands

Namespaces-You can think of each namespace as a folder that holds a set of objects. By default, the kubectl command- line tool interacts with the default namespace. If you want to use a different namespace, you can pass kubectl the --namespace flag. For example, kubectl -- namespace=mystuff references objects in the mystuff namespace

Contexts-If you want to change the default namespace more permanently, you can use a context. This gets recorded in a kubectl configuration file, usually located at $HOME/.kube/config
	- Context is like namespace but more permanent; can have diff namespace/context: $ kubectl config set-context my-context --namespace=mystuff
	- In oder to USE context, must be imperatively expressed: $ kubectl config use-context my-context

Another common task is extracting specific fields from the object: $ kubectl get pods my-pod -o jsonpath --template={.status.podIP}
	
# Let’s assume that you have a simple object stored in obj.yaml. You can use kubectl to create this object in Kubernetes by running: #
	- $ kubectl apply -f obj.yaml

# to add the color=red label to a pod named bar, you can run: #
$ kubectl label pods bar color=red

NOTE By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the --overwrite flag. NOTE

# If you want to remove a label, you can use the -<label-name> syntax: #
$ kubectl label pods bar -color

# You can also use the exec command to execute a command in a running container: #
$ kubectl exec -it <pod-name> -- bash
	- This will provide you with an interactive shell inside the running container so that you can perform more debugging.

$ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
	- This will copy a file from a running container to your local machine.

***********************************************************************************************************************

Chapter 5 Pods

A Pod represents a collection of application containers and volumes running in the same execution environment.

Each container within a Pod runs in its own cgroup, but they share a number of Linux namespaces.

Applications running in the same Pod share the same IP address and port space (network namespace), have the same hostname (UTS namespace)...

However, applications in different Pods are isolated from each other; they have different IP addresses, different hostnames, and more. 

Containers in different Pods running on the same node might as well be on different servers.

Kubernetes strongly believes in declarative configuration. 
	- Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state.

The simplest way to create a Pod is via the imperative kubectl run command
	- $ kubectl run kuard --image=gcr.io/kuar-demo/kuard-amd64:1


Example 5-1. kuard-pod.yaml_______________________________________________

apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
    name: kuard
    ports:- containerPort: 8080
      name: http
      protocol: TCP

*** to run this pod: $ kubectl apply -f kuard-pod.yaml ***
___________________________________________________________________________

Mapping local machine port through the Kubernetes master, to the instance of the Pod running on one of the worker nodes
	- $ kubectl port-forward kuard 8080:8080

Coping files from pod/container to local
	- $ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt

Coping files from local to pod/container
	- $ kubectl cp $HOME/config.txt <pod-name>:/config.txt

Liveness Probe- Liveness determines if an application is running properly
	tcpSocket health check- Attempts to open 3-way handshake 
Heath Check and Liveness Probe Example____________________________________

apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
^^       ^^        ^^        ^^        ^^
The preceding Pod manifest uses an *httpGet* probe to perform an HTTP GET request 
against the */healthy* endpoint on port *8080* of the kuard container. The probe sets
an *initialDelaySeconds of 5*, and thus will not be called until five seconds after
all the containers in the Pod are created. The probe must respond within the *one-
second timeout*, and the HTTP status code must be equal to or *greater than 200 and
less than 400* to be considered successful. Kubernetes will call the probe every *10
seconds*. If more than *three* probes fail, the container will fail and restart.

^^
This command created the pod:  $ kubectl apply -f kuard-pod-health.yaml
This command does port-forwarding to the Pods: $ kubectl port-forward kuard 8080:8080


Readiness Probe- Readiness describes when a container is ready to serve user requests.
	- Containers that fail readiness checks are removed from service load balancers.

Exec probe- runs customable execute script to probe specific applications

REQUEST GAURENTEED
Example 5-3. kuard-pod-resreq.yaml__________________________________________________________
apiVersion: v1kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
    name: kuard
    # Resource request here gaurantees 500m for RAM, and 128M of disk space 
    resources:
      requests:
        cpu: "500m"
        memory: "128Mi"
    ports:
      - containerPort: 8080
        name: http
        protocol: TCP


CAPPING RESOURCE LIMITS
Example 5-4. kuard-pod-reslim.yaml________________________________________________
apiVersion: v1
kind: Pod
  metadata:
name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      ports:
        - containerPort: 8080
        name: http
        protocol: TCP     


PERSISTING DATA VOLUMES
	- To add a volume to a Pod manifest, there are two new stanzas to add to our configuration. The first is a new spec.volumes section. 
	  This array defines all of the volumes that may be accessed by containers in the Pod manifest.
	  
	- The second addition is the volumeMounts array in the container definition. This array defines the volumes that are mounted
	  into a particular container, and the path where each volume should be mounted  
	  
	  
Example 5-5. kuard-pod-vol.yaml_______________________________________________________
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
# This array defines all of the volumes that may be accessed by containers in the Pod manifest
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/var/lib/kuard"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
# This array defines the volumes that are mounted into a particular container, and the path where each volume should be mounted
        volumeMounts:
          - mountPath: "/data" 
            name: "kuard-data" 
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
	  
"emptyDir" is a way for two containers to share volume resources.

Persisting Data Using Remote Disks
..............................
volumes:
  - name: "kuard-data"
    nfs:
      server: my.nfs.server.local
      path: "/exports"
...............................

Putting it all together

Example 5-6. kuard-pod-full.yaml_____________________________________
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        resources:
          requests:
            cpu: "500m"
            memory: "128Mi"
          limits:
            cpu: "1000m"
	    memory: "256Mi"
	volumeMounts:
          - mountPath: "/data"
            name: "kuard-data"
        livenessProbe:
          httpGet:
            path: /healthy
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
          path: /ready
          port: 8080
        initialDelaySeconds: 30
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3

***********************************************************************************************************************
Chapter 6 Labels and Annotations 

labels="ver=2,app=alpaca,env=test"
	- In this example, labels are being set: ver=2; app=alpaca; env=test
	- List rs/deployments and their label info: $ kubectl get deployments --show-labels
	- Modifying labels imperatively: $ kubectl label deployments alpaca-test "canary=true"
	
Label Selector
	- $ kubectl get pods --selector="ver=2"
	- $ kubectl get pods --selector="app=bandicoot,ver=2"
	- $ kubectl get pods --selector="app in (alpaca,bandicoot)"
	

Annotation
	- Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries.
	- Can be used for the tool itself or to pass configuration information between external systems.
	- While labels are used to identify and group objects, annotations are used to provide extra information about where an object came from, how to use it, or policy around that object.
	- Keep track of a “reason” for the latest update to an object.
	- Communicate a specialized scheduling policy to a specialized scheduler.
	- Extend data about the last tool to update the resource and how it was updated (used for detecting changes by other tools and doing a smart merge).
	- Build, release, or image information that isn’t appropriate for labels (may include a Git hash, timestamp, PR number, etc.).
	- Enable the Deployment object (Chapter 12) to keep track of ReplicaSets that it is managing for rollouts.
	- Provide extra data to enhance the visual quality or usability of a UI. For example, objects could include a link to an icon (or a base64-encoded version of an icon).
	- Prototype alpha functionality in Kubernetes (instead of creating a first-class API field, the parameters for that functionality are instead encoded in an annotation).
	
Defining Annotations Example:
...
metadata:
  annotations:
    example.com/icon-url: "https://example.com/icon.png"
...


***********************************************************************************************************************
Chapter 7 Service Discovery (DNS for Kubernetes)

What is service discovery- Service discovery tools help solve the problem of finding which processes are listening at which addresses for which services.

Service Object:
	A Service object is a way to create a named label selector.

$ kubectl describe service alpaca-prod
Name:			alpaca-prod
Namespace:		default
Labels:			app=alpaca
			env=prod
			ver=1
Annotations:		<none>
Selector:		app=alpaca,env=prod,ver=1
Type:			NodePort (This can also be type 'LoadBalancer')
IP:			10.115.245.13
Port:			<unset> 8080/TCP
NodePort:		<unset> 32711/TCP
Endpoints:		10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080
Session Affinity:	None
No events.

Endpoints
	- Some applications (and the system itself) want to be able to use services without using a cluster IP. (It uses 'EndPoints')
	- For every Service object, Kubernetes creates a buddy Endpoints object that contains the IP addresses for that service
	- To use a service, an advanced application can talk to the Kubernetes API directly to look up endpoints and call them

Kubernetes services are built on top of label selectors over pods. That means that you can use the Kubernetes API to do rudimentary 
service discovery without using a Service object at all.

***********************************************************************************************************************
Chapter 8 ReplicaSets

Reconciliation Loops- The central concept behind a reconciliation loop is the notion of desired state and observed or current state. Desired state is the state you want.
	- The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state.
	
ReplicaSets	
	- Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing.
	- Can adopt existing pods as opposed to deleting a pod and relaunching it via a ReplicaSet
	- ReplicaSets are designed to represent a single, scalable microservice inside your architecture.

Quarantining Containers
	- You can modify the set of labels on the sick Pod; doind so will disassociate it from the ReplicaSet and service so you can debug the pod.
	

Example 8-1. kuard-rs.yaml_______________________________

apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: kuard
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kuard
        version: "2"
  spec:
    containers:
      - name: kuard
      image: "gcr.io/kuar-demo/kuard-amd64:2"

Obtain details for ReplicaSet [name]: $ kubectl describe rs [metadata.name]

Finding a Set of Pods for a ReplicaSet: $ kubectl get pods -l app=kuard,version=2

Imperative Scaling ReplicaSets [spec.replicas]: $ kubectl scale kuard --replicas=4

Scale ReplicaSet based on CPU utilization (percentage): $ kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80
	- This command creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%

Horizontal Pod Autoscaling (HPA)

^^^^
Kubernetes makes a distinction between horizontal scaling, which involves creating additional replicas of a Pod, and vertical scaling,
which involves increasing the resources required for a particular Pod (e.g., increasing the CPU required for the Pod)

Web server like nginx, you may want to scale due to CPU usage. For an in-memory cache, you may want to scale with memory consumption. In some
cases you may want to scale in response to custom application metrics, hence HPA

!NOTE----NOTE----NOTE!

HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling
decisions. Most installations of Kubernetes include heapster by default. You can validate its presence by listing the Pods in the kube-system namespace:
	- $ kubectl get pods --namespace=kube-system

You should see a Pod named heapster somewhere in that list. If you do not see it,
autoscaling will not work correctly.

!NOTE----NOTE----NOTE!

***********************************************************************************************************************
Chapter 9 DaemonSets (DaemonSets put a pod on each node)

By default a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of
labels.

Creating DaemonSets

Example 9-1. fluentd.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:v0.14.10
        resources:
          limits:
            memory: 200Mi
        requests:
          cpu: 100m
          memory: 200Mi
      volumeMounts:
      - name: varlog
        mountPath: /var/log
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers

By default, DaemonSets runs a pod on every node in the cluster. In order to limit this feature by DaemonSet, use "Labels" for
descriptors/selectors to the nodes you want DaemonSet to run on. For example, here's how you label a single node:
	- $ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true
	
	
The following DaemonSet configuration limits nginx to running only on nodes w/ 'ssd=true'

Example 9-2. nginx-fast-storage.yaml____________________________________

apiVersion: extensions/v1beta1
kind: "DaemonSet"
  metadata:
    labels:
      app: nginx
      ssd: "true"
    name: nginx-fast-storage
spec:
  template:
    metadata:
      labels:
        app: nginx
        ssd: "true"
    spec:
      nodeSelector:
        ssd: "true"
      containers:
        - name: nginx
          image: nginx:1.10.0

How will we update DaemonSets? Deleting and re-deploying? Or a rollning update for DaemonSets?
	- Deletion and re-deployment could have too much downtime
	- W/Kubernetes 1.6, DaemonSets can now be rolled out using the same rolling update strategy that deploments use
	- To set a DaemonSet to use the rolling update strategy, you need to configure the update strategy using the spec.updateStrategy.type field
	- When a DaemonSet has an update strategy of RollingUpdate, any change to the spec.template field(or subfields)in he DaemonSet will initiate the roll out.

2 parameters that control the rolling update:
	- spec.minReadySeconds- determines how long Pod must be “ready” before the rolling update proceeds to next Pods
	- spec.updateStrategy.rollingUpdate.maxUnavailable- indicates how many Pods may be simultaneously updated by the rolling update


***********************************************************************************************************************
Chapter 10 Jobs

Job: 
	- short lived, one-off tasks
	- These pods generally run until successful completion 
	- The Job object coordinates running a number of pods in parallel
	- If Pod fails before a successful termination, Job controller creates new Pod based on Pod template in Job specification
	  
Two primary attributes of a Job:
	- The number of job completions
	- The number of pods to run parallel 
	
"Run once until completion" pattern hasn 'completions' and 'parallelism' set to '1'

On Shot -> Database Migrations -> Single pod running until successful term -> completions:1 & parallelism:1
	- Run single pod once until successfil term.

Parallel fixed Compl. -> multiple pods processing set of work in paralell + ge 1 pod(s) running ge 1 until reaching fixed completion count -> completion:1+ & paralellism 1+

Work que/parelell jobs -> multiple pods processing from centralized work queue -> ge 1 pods running once until successful -> completions:1 & paralellism: ge2


One Shot- provide a way to run a single Pod once until successful termination.
	- 
Example 10-1. job-oneshot.yaml____________________________________________
apiVersion: batch/v1
kind: Job
metadata:
  name: oneshot
  labels:
    chapter: jobs
spec:
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:1
        imagePullPolicy: Always
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-num-to-gen=10"
      restartPolicy: OnFailure

**Note on above YAML**
--restart=OnFailure is the option that tells kubectl to create a Job object.


Example 10-3. job-parallel.yaml______________________________
apiVersion: batch/v1
kind: Job
metadata:
  name: parallel
  labels:
    chapter: jobs
spec:
  parallelism: 5
  completions: 10
  template:
    metadata:
      labels:
        chapter: jobs
  spec:
    containers:
    - name: kuard
      image: gcr.io/kuar-demo/kuard-amd64:1
      imagePullPolicy: Always
      args:
      - "--keygen-enable"
      - "--keygen-exit-on-complete"
      - "--keygen-num-to-gen=10"
    restartPolicy: OnFailure  
    
    
Work Queue- A common use case for Jobs is to process work from a work queue. In this scenario, some task creates a number of work items and 
publishes them to a work queue. A worker Job can be run to process each work item until the work queue is empty.

Work queue example:

Example 10-4. rs-queue.yaml_____________________________________________________
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  labels:
    app: work-queue
    component: queue
    chapter: jobs
  name: queue
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: work-queue
        component: queue
        chapter: jobs
  spec:
    containers:
    - name: queue
      image: "gcr.io/kuar-demo/kuard-amd64:1"
      imagePullPolicy: Always
      

Next, loading up the queue
Once the work queue is in place, we should expose it using a service. This will make it easy for prods and customers to locate
work queue via DNS:


Example 10-5. service-queue.yaml___________________________________________
apiVersion: v1
kind: Service
metadata:
  labels:
    app: work-queue
    component: queue
    chapter: jobs
  name: queue
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: work-queue
    component: queue
    
              ^^^^^^^^^^^^^^^^^^^^^^^
Create the queue service with kubectl:
$ kubectl apply -f service-queue.yaml


LOADING UP THE QUEUE
	- Using curl to drive the API for the work queue server and insert work items
	- curl communicates to the work queue through the 'kubectl port-forward' that was set up earlier:

Example 10-6. load-queue.sh
# Create a work queue called 'keygen'
curl -X PUT localhost:8080/memq/server/queues/keygen

# Create 100 work items and load up the queue.
for i in work-item-{0..99}; do
  curl -X POST localhost:8080/memq/server/queues/keygen/enqueue \
    -d "$i"
done

(Run these commands, and you should see 100 JSON objects output to your terminal. You can confirm the status of the queue by looking at the “MemQ Server” tab in the UI)
or you can ask the work queue API directly:

$ curl 127.0.0.1:8080/memq/server/stats
{
	"kind": "stats",
	"queues": [
	    {
		"depth": 100,
		"dequeued": 0,
		"drained": 0,
		"enqueued": 100,
		"name": "keygen"
	    }
	]
}


Set up to draw work items from the work queue create a key, and then exit once the queue is empty:

Example 10-7. job-consumers.yaml_________________________________

apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: message-queue
    component: consumer
    chapter: jobs
  name: consumers
spec:
  parallelism: 5
template:
  metadata:
    labels:
      app: message-queue
      component: consumer
      chapter: jobs
  spec:
    containers:
    - name: worker
      image: "gcr.io/kuar-demo/kuard-amd64:1"
      imagePullPolicy: Always
      args:
      - "--keygen-enable"
      - "--keygen-exit-on-complete"
      - "--keygen-memq-server=http://queue:8080/memq/server"
      - "--keygen-memq-queue=key  
    restartPolicy: OnFailure
    
		^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We are telling the Job to start up five pods in parallel. As the completions parameter is unset, we put the Job into a worker pool mode. Once the first pod exits
with a zero exit code, the Job will start winding down and will not start any new Pods. This means that none of the workers should exit until the work is done and
they are all in the process of finishing up.

Create the consumers Job:
	- $ kubectl apply -f job-consumers.yaml
	
	
***********************************************************************************************************************
Chapter 11 ConfigMaps and Secrets

ConfigMap- think of ConfigMap as a Kube object that defines a small filesystem. Another way is as a set of variables that can be used when defining the
environment or command line for your containers.

This example assumes that you have a file called 'my-config.txt':
|   --# This is a sample config file that I might use to configure an application
|   --parameter1 = value1
|   --parameter2 = value2

$ kubectl create configmap my-config \
--from-file=my-config.txt \
--from-literal=extra-param=extra-value \
--from-literal=another-param=another-value

OR

| This is the equivelant declaritive | 

apiVersion: v1
data:
  another-param: another-value
  extra-param: extra-value
  my-config.txt: |
    # This is a sample config file that I might use to configure an application
    parameter1 = value1
    parameter2 = value2
kind: ConfigMap
metadata:
  creationTimestamp: ...
  name: my-config
  namespace: default
  resourceVersion: "13556"
  selfLink: /api/v1/namespaces/default/configmaps/my-config
  uid: 3641c553-f7de-11e6-98c9-06135271a273

*ConfigMap is really just some key/value pairs stored in an object

USING A CONFIGMAP (3 ways to use configMap)
- Filesystem
	- mount a ConfigMap into a Pod. A file is created for each entry based on key name. contents of file are set to the value.
- Environment Var
	- A ConfigMap can be used to dynamically set the value of an environment variable.
- CLI argument
	- Command-line arguments build on environment variables. Kubernetes will perform the correct substitution 
	  with a special $(<env-var-name>) syntax.

--Let’s create a manifest for kuard that pulls all of these together, as shown in--
Example 11-2. kuard-config.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kuard-config
spec:
  containers:
    - name: test-container
      image: gcr.io/kuar-demo/kuard-amd64:1
      imagePullPolicy: Always
      command:
        - "/kuard"
        - "$(EXTRA_PARAM)"
      env:
        - name: ANOTHER_PARAM
          valueFrom:
            configMapKeyRef:
            name: my-config
            key: another-param
       -  name: EXTRA_PARAM
          valueFrom:
            configMapKeyRef:
            name: my-config
            key: extra-param
      volumeMounts:
        - name: config-volume
          mountPath: /config
  volumes:
    - name: config-volume
      configMap:
        name: my-config
    restartPolicy: Never


For the filesystem method, we create a new volume inside the pod and give it the name config-volume. We then define this 
volume to be a ConfigMap volume and point at the ConfigMap to mount. We have to specify where this gets mounted into
the kuard container with a volumeMount. In this case we are mounting it at /config.

Environment variables are specified with a special valueFrom member. This references the ConfigMap and the data key to use 
within that ConfigMap.

						--->SECRETS<-----
- enable container images to be created without bundling sensitive data
- This allows containers to remain portable across environments.
- are exposed to pods via explicit declaration in pod manifests and the Kubernetes API
- The first step in creating a secret is to obtain the raw data we want to store (store .crt & .key locally)

$ kubectl create secret generic <name>-tls \
--from-file=kuard.crt \
--from-file=kuard.key

(get details by using:  $ kubectl describe secrets kuard-tls)

					---->CONSUMING SECRETS<------
		
- Secrets can be consumed using the Kubernetes REST API by applications that know how to call that API directly.
- Instead of accessing secrets through the API server, we can use a secrets volume.

- Secrets are stored on tmpfs volumes (aka RAM disks) and, as such, are not written to disk on nodes.
- Each data element of a secret is stored in a separate file under the target mount point specified in the volume mount.

The following pod manifest (Example 11-3) demonstrates how to declare a secrets volume

Example 11-3. kuard-secret.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kuard-tls
spec:
  containers:
    - name: kuard-tls
      image: gcr.io/kuar-demo/kuard-amd64:1
      imagePullPolicy: Always
      volumeMounts:
      - name: tls-certs
        mountPath: "/tls"
        readOnly: true
volumes:
  - name: tls-certs
    secret:
      secretName: kuard-tls
      
^^^^^^^^^^^^^^^^^^^^^^^^^^^      
Connect to the pod by running:
$ kubectl port-forward kuard-tls 8443:8443


			---->PRIVATE DOCKER REGISTRIES<-----
- A special use case for secrets is to store access credentials for private Docker registries.
- Kubernetes supports using images stored on private registries, but access to those images requires credentials.
- Image pull secrets leverage the secrets API to automate the distribution of private registry credentials.
- Image pull secrets are stored just like normal secrets but are consumed through the spec.imagePullSecrets Pod specification field.
	^^^^^^
$ kubectl create secret docker-registry my-image-pull-secret \
--docker-username=<username> \
--docker-password=<password> \
--docker-email=<email-address>

Example 11-4. kuard-secret-ips.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kuard-tls
spec:
  containers:
    - name: kuard-tls
      image: gcr.io/kuar-demo/kuard-amd64:1
      imagePullPolicy: Always
      volumeMounts:
      - name: tls-certs
        mountPath: "/tls"
        readOnly: true
  imagePullSecrets:
    - name: my-image-pull-secret
    volumes:
      - name: tls-certs
        secret:
          secretName: kuard-tls

This will let you see raw data: $ kubectl get secret kuardtls -o yaml

*kubectl create secret generic or kubectl create configmap* 

--from-file=<filename>
Load from the file with the secret data key the same as the filename.

--from-file=<key>=<filename>
Load from the file with the secret data key explicitly specified.

--from-file=<directory>
Load all the files in the specified directory where the filename is an acceptable
key name.

--from-literal=<key>=<value>
Use the specified key/value pair directly.

*you can just edit it directly and push a new version with kubectl replace -f <filename>.*


				---->Recreate and update<-----

If you store the inputs into your ConfigMaps or secrets as separate files on disk (opposed to embedded into YAML directly), 
you can use kubectl to recreate the manifest and then use it to update the object.
This will look something like this:

$ kubectl create secret generic kuard-tls \
--from-file=kuard.crt --from-file=kuard.key \
--dry-run -o yaml | kubectl replace -f -
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This command line first creates a new secret with the same name as our existing secret.we tell kubectl not to actually send 
the data to the server but instead to dump the YAML that it would have sent to the API server to stdout.


***********************************************************************************************************************

Chapter 12 Deployments

- ReplicaSets manage Pods, Deployments manage ReplicaSets

An example of a rollout using a deployment "kind":

apiVersion: extensions/v1beta1
kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    labels:
      run: nginx
    name: nginx
    namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx:1.7.12
        imagePullPolicy: Always
      dnsPolicy: ClusterFirst
      restartPolicy: Always

  : IMPORTANT NOTE :
  
The best practice is to manage your Deployments declaratively via the YAML files

  : IMPORTANT NOTE :
  
 To see "REVISION" and "CHANGE CAUSE (aka annotation)" for Deployment history, use:
	- $ kubectl rollout history deployment <name>
	
To undo to a previous version
	- $ kubectl rollout undo deployments <name> --to-revision=<ver #>
	
				------------->Deployment Strategies<------------------
1. Recreate
	- RS updates the image and terminates the RS that it manages. RS notices desired state not satisfied, so spins up new ones with updated image
	- Down side is that this method has downtime.

2. RollingUpdate
	- you can roll out a new version of your service while it is still receiving user traffic for the older, without any downtime.
	- important to note that complications arrive depending on how software is built
		- for example, ver 1 can use lib 1, and ver 2 can use lib 2. User can already be using ver 1 on a pod that has been 
		  since updated to ver 2, so a "lib request" will go to lib 2. This may cause conflict, ver 1 againt lib 2. (Page 156)
		- A backend load balancer can mitigate this issue (page 157).
2a. Configuring RollingUpdate
	- MaxUnavailable
		- parameter sets the maximum number of Pods that can be unavailable during a rolling update.
		- It can either be set to an absolute number or by percentage 
	- MaxSurge 
		- parameter sets surge % of updated pods to spin up before proceeding w/the RollingUpdate
		
Ensuring Health During RollingUpdate____________
- minReadySeconds
	- indicates that the Deployment must wait for x seconds after seeing a Pod become healthy before moving on to updating the next Pod
- progressDeadlineSeconds
	- If any particular stage in the rollout fails to progress in 10 minutes, then the Deployment is marked as failed, and all 
	  attempts to move the Deployment forward are halted.
	  


***********************************************************************************************************************
Chapter 13 Integrating Storage Solutions and Kubernetes


			            ------->Importing External Services<----------
				    
- You have an existing machine running in your network that has some sort of database running on it. In this situation you 
  may not want to immediately move that database into containers and Kubernetes.
  
- Take advantage of all of the built-in naming and service discovery primitives provided by Kubernetes. Additionally, this
  enables you to configure all your applications so that it looks like the database that is running on a machine somewhere is actually 
  a Kubernetes service

- When you deploy a Pod into the test namespace and it looks up the service named my-database, it will receive a pointer to 
  my-database.test.svc.cluster.internal

- when a Pod deployed in the prod namespace looks up the same name (my-database) it will receive a pointer to mydatabase.
  prod.svc.cluster.internal

StatefulSets


Automating MongoDB Cluster Creation
Manually replicated mongo db with stateful sets

***********************************************************************************************************************

Chapter 14 Deploying Real-World Apps

Three examples used in this chapter:

Parse, an open source API server for mobile applications
	- replicate MongoDB using Kube StatefulSets
	- 1st, clone the Parse repo: git clone https://github.com/ParsePlatform/parse-server

Ghost, a blogging and content management platform

Redis, a lightweight, performant key/value store
	- consist of two programs: redis-server and redis-sentinel
		- redis-servers: slave (replica RO for LB) and master (RW)
		- redis-sentinel: health check and failovers

	- 3 config files are needed: master, slave and sentinel
	- 2 bash files: init.sh (to determine master/slave config to use on which server) and sentinel.sh for sentinel 
	- redis service needs to be created w/yaml file (w/o a cluster IP address)


Stateful vs Stateless


Deploying Parse server need three environment variables:
APPLICATION_ID
An identifier for authorizing your application

MASTER_KEY
An identifier that authorizes the master (root) user

DATABASE_URI
The URI for your MongoDB cluster
















							Misc Notes
				
*The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app.*
				
Naming and DNS is done using "kind: Service" yaml


YAML for exposing Kube service:

apiVersion: v1
kind: Service
metadata:
  name: parse-server
  namespace: default
spec:
  ports:
    - port: 1337
      protocol: TCP
      targetPort: 1337
  selector:
    run: parse-server




Example 14-11. redis.yaml

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: redis
spec:
  replicas: 3
  serviceName: redis
  template:
    metadata:
      labels:
        app: redis
  spec:
    containers:
    - command: [sh, -c, source /redis-config/init.sh ]
      image: redis:3.2.7-alpine
      name: redis
      ports:
      - containerPort: 6379
        name: redis
      volumeMounts:
      - mountPath: /redis-config
        name: config
      - mountPath: /redis-data
       name: data
    - command: [sh, -c, source /redis-config/sentinel.sh]
      image: redis:3.2.7-alpine
      name: sentinel
      volumeMounts:
      - mountPath: /redis-config
        name: config
      volumes:
      - configMap:
        defaultMode: 420
        name: redis-config
      name: config
    - emptyDir:
      name: data

^^NOTE^^
You can also note that there are two volumes defined in the yaml. One is the volume
that uses our ConfigMap to configure the two Redis applications, and the other is a
simple emptyDir volume that is mapped into the Redis server container to hold the
application data so that it survives a container restar
