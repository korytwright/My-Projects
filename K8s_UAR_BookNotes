Chapter 1

Terms:

Cluster- one or many pods

Pod-is a group of applications providing the same service (replicas)

Node-is a physical device that house the pod(s)

Velocity-is measured not in terms of the raw number of features you can ship per hour or day, but rather in terms of the number of things you can ship while maintaining a highly available service.

Scaling (of both software and teams)

Abstracting your infrastructure

Efficiency

Decoupling-making sure front-end and back-end are completely seperated; front end would use API to comminucate with backend.

*****************************

Chapter 2 Creating and Running Containers

Docker consist of images and containers.
	- And image can either be created by a Dockerfile, or is pulled down from a repo
	- Docker containers are created from Docker images (the evironment and variable declared inside of the Dockerfile)

A container can either be a system container, or an application container.
	- System containers seek to mimic virtual machines and often run a full boot process. They often include a set of system services typically found in a VM, such as ssh, cron, and syslog
	- Application containers commonly run a single application

Container images are constructed of a series of filesystem layers, where each layer inherits and modifies the layer that came before it.

Container images are typically combined with a container configuration file, which provides instructions on how to set up the container environment and execute anapplication entrypoint

The container root filesystem and configuration file are typically bundled using the Docker image format.

Fork - Is when you build off of a container image and it is independant of the parenet image (starting its own branch).



*****************************

Chapter 3 Installing Kubes on AWS, Google Cloud Platform, Microsoft Azure and Raspberry Pi


$ kubectl get nodes (list all nodes in the cluster)

$ kubectl describe nodes node-1 (see basic information about a specific node)

Cluster components:
Kube Proxy-is responsible for routing network traffic to load-balancedservices in the Kubernetes cluster (ran using Kubernetes DaemonSet).

Kube DNS-which provides naming and discovery for the services that are defined in the cluster (ran using Kubernetes deployment).

Kuberenets UI is ran using deployment; get information about UI dashboard: $ kubectl get deployments --namespace=kube-system kubernetes-dashboard

Everything contained in Kubernetes is represented by a RESTful resource (API).

*****************************

Chapter 4 Common kubectl Commands

Namespaces-You can think of each namespace as a folder that holds a set of objects. By default, the kubectl command- line tool interacts with the default namespace. If you want to use a different namespace, you can pass kubectl the --namespace flag. For example, kubectl -- namespace=mystuff references objects in the mystuff namespace

Contexts-If you want to change the default namespace more permanently, you can use a context. This gets recorded in a kubectl configuration file, usually located at $HOME/.kube/config
	- Context is like namespace but more permanent; can have diff namespace/context: $ kubectl config set-context my-context --namespace=mystuff
	- In oder to USE context, must be imperatively expressed: $ kubectl config use-context my-context

Another common task is extracting specific fields from the object: $ kubectl get pods my-pod -o jsonpath --template={.status.podIP}
	
# Let’s assume that you have a simple object stored in obj.yaml. You can use kubectl to create this object in Kubernetes by running: #
	- $ kubectl apply -f obj.yaml

# to add the color=red label to a pod named bar, you can run: #
$ kubectl label pods bar color=red

NOTE By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the --overwrite flag. NOTE

# If you want to remove a label, you can use the -<label-name> syntax: #
$ kubectl label pods bar -color

# You can also use the exec command to execute a command in a running container: #
$ kubectl exec -it <pod-name> -- bash
	- This will provide you with an interactive shell inside the running container so that you can perform more debugging.

$ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
	- This will copy a file from a running container to your local machine.

*****************************

Chapter 5 Pods

A Pod represents a collection of application containers and volumes running in the same execution environment.

Each container within a Pod runs in its own cgroup, but they share a number of Linux namespaces.

Applications running in the same Pod share the same IP address and port space (network namespace), have the same hostname (UTS namespace)...

However, applications in different Pods are isolated from each other; they have different IP addresses, different hostnames, and more. 

Containers in different Pods running on the same node might as well be on different servers.

Kubernetes strongly believes in declarative configuration. 
	- Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state.

The simplest way to create a Pod is via the imperative kubectl run command
	- $ kubectl run kuard --image=gcr.io/kuar-demo/kuard-amd64:1


Example 5-1. kuard-pod.yaml_______________________________________________

apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
    name: kuard
    ports:- containerPort: 8080
      name: http
      protocol: TCP

*** to run this pod: $ kubectl apply -f kuard-pod.yaml ***
___________________________________________________________________________

Mapping local machine port through the Kubernetes master, to the instance of the Pod running on one of the worker nodes
	- $ kubectl port-forward kuard 8080:8080

Coping files from pod/container to local
	- $ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt

Coping files from local to pod/container
	- $ kubectl cp $HOME/config.txt <pod-name>:/config.txt

Liveness Probe- Liveness determines if an application is running properly
	tcpSocket health check- Attempts to open 3-way handshake 
Heath Check and Liveness Probe Example____________________________________

apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
^^       ^^        ^^        ^^        ^^
The preceding Pod manifest uses an *httpGet* probe to perform an HTTP GET request 
against the */healthy* endpoint on port *8080* of the kuard container. The probe sets
an *initialDelaySeconds of 5*, and thus will not be called until five seconds after
all the containers in the Pod are created. The probe must respond within the *one-
second timeout*, and the HTTP status code must be equal to or *greater than 200 and
less than 400* to be considered successful. Kubernetes will call the probe every *10
seconds*. If more than *three* probes fail, the container will fail and restart.

^^
This command created the pod:  $ kubectl apply -f kuard-pod-health.yaml
This command does port-forwarding to the Pods: $ kubectl port-forward kuard 8080:8080


Readiness Probe- Readiness describes when a container is ready to serve user requests.
	- Containers that fail readiness checks are removed from service load balancers.

Exec probe- runs customable execute script to probe specific applications

REQUEST GAURENTEED
Example 5-3. kuard-pod-resreq.yaml__________________________________________________________
apiVersion: v1kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
    name: kuard
    # Resource request here gaurantees 500m for RAM, and 128M of disk space 
    resources:
      requests:
        cpu: "500m"
        memory: "128Mi"
    ports:
      - containerPort: 8080
        name: http
        protocol: TCP


CAPPING RESOURCE LIMITS
Example 5-4. kuard-pod-reslim.yaml________________________________________________
apiVersion: v1
kind: Pod
  metadata:
name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      ports:
        - containerPort: 8080
        name: http
        protocol: TCP     


PERSISTING DATA VOLUMES
	- To add a volume to a Pod manifest, there are two new stanzas to add to our configuration. The first is a new spec.volumes section. 
	  This array defines all of the volumes that may be accessed by containers in the Pod manifest.
	  
	- The second addition is the volumeMounts array in the container definition. This array defines the volumes that are mounted
	  into a particular container, and the path where each volume should be mounted  
	  
	  
Example 5-5. kuard-pod-vol.yaml_______________________________________________________
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
# This array defines all of the volumes that may be accessed by containers in the Pod manifest
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/var/lib/kuard"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
# This array defines the volumes that are mounted into a particular container, and the path where each volume should be mounted
        volumeMounts:
          - mountPath: "/data" 
            name: "kuard-data" 
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
	  
"emptyDir" is a way for two containers to share volume resources.

Persisting Data Using Remote Disks
..............................
volumes:
  - name: "kuard-data"
    nfs:
      server: my.nfs.server.local
      path: "/exports"
...............................

Putting it all together

Example 5-6. kuard-pod-full.yaml_____________________________________
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        resources:
          requests:
            cpu: "500m"
            memory: "128Mi"
          limits:
            cpu: "1000m"
	    memory: "256Mi"
	volumeMounts:
          - mountPath: "/data"
            name: "kuard-data"
        livenessProbe:
          httpGet:
            path: /healthy
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
          path: /ready
          port: 8080
        initialDelaySeconds: 30
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3

*****************************
Chapter 6 Labels and Annotations 

labels="ver=2,app=alpaca,env=test"
	- In this example, labels are being set: ver=2; app=alpaca; env=test
	- List rs/deployments and their label info: $ kubectl get deployments --show-labels
	- Modifying labels imperatively: $ kubectl label deployments alpaca-test "canary=true"
	
Label Selector
	- $ kubectl get pods --selector="ver=2"
	- $ kubectl get pods --selector="app=bandicoot,ver=2"
	- $ kubectl get pods --selector="app in (alpaca,bandicoot)"
	

Annotation
	- Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries.
	- Can be used for the tool itself or to pass configuration information between external systems.
	- While labels are used to identify and group objects, annotations are used to provide extra information about where an object came from, how to use it, or policy around that object.
	- Keep track of a “reason” for the latest update to an object.
	- Communicate a specialized scheduling policy to a specialized scheduler.
	- Extend data about the last tool to update the resource and how it was updated (used for detecting changes by other tools and doing a smart merge).
	- Build, release, or image information that isn’t appropriate for labels (may include a Git hash, timestamp, PR number, etc.).
	- Enable the Deployment object (Chapter 12) to keep track of ReplicaSets that it is managing for rollouts.
	- Provide extra data to enhance the visual quality or usability of a UI. For example, objects could include a link to an icon (or a base64-encoded version of an icon).
	- Prototype alpha functionality in Kubernetes (instead of creating a first-class API field, the parameters for that functionality are instead encoded in an annotation).
	
Defining Annotations Example:
...
metadata:
  annotations:
    example.com/icon-url: "https://example.com/icon.png"
...


*****************************
Chapter 7 Service Discovery (DNS for Kubernetes)

What is service discovery- Service discovery tools help solve the problem of finding which processes are listening at which addresses for which services.

Service Object:
	A Service object is a way to create a named label selector.

$ kubectl describe service alpaca-prod
Name:			alpaca-prod
Namespace:		default
Labels:			app=alpaca
			env=prod
			ver=1
Annotations:		<none>
Selector:		app=alpaca,env=prod,ver=1
Type:			NodePort (This can also be type 'LoadBalancer')
IP:			10.115.245.13
Port:			<unset> 8080/TCP
NodePort:		<unset> 32711/TCP
Endpoints:		10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080
Session Affinity:	None
No events.

Endpoints
	- Some applications (and the system itself) want to be able to use services without using a cluster IP. (It uses 'EndPoints')
	- For every Service object, Kubernetes creates a buddy Endpoints object that contains the IP addresses for that service
	- To use a service, an advanced application can talk to the Kubernetes API directly to look up endpoints and call them

Kubernetes services are built on top of label selectors over pods. That means that you can use the Kubernetes API to do rudimentary 
service discovery without using a Service object at all.

*****************************
Chapter 8 ReplicaSets

Reconciliation Loops- The central concept behind a reconciliation loop is the notion of desired state and observed or current state. Desired state is the state you want.
	- The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state.
	
ReplicaSets	
	- Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing.
	- Can adopt existing pods as opposed to deleting a pod and relaunching it via a ReplicaSet
	- ReplicaSets are designed to represent a single, scalable microservice inside your architecture.

Quarantining Containers
	- You can modify the set of labels on the sick Pod; doind so will disassociate it from the ReplicaSet and service so you can debug the pod.
	

Example 8-1. kuard-rs.yaml_______________________________

apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: kuard
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kuard
        version: "2"
  spec:
    containers:
      - name: kuard
      image: "gcr.io/kuar-demo/kuard-amd64:2"

Obtain details for ReplicaSet [name]: $ kubectl describe rs [metadata.name]

Finding a Set of Pods for a ReplicaSet: $ kubectl get pods -l app=kuard,version=2

Imperative Scaling ReplicaSets [spec.replicas]: $ kubectl scale kuard --replicas=4

Scale ReplicaSet based on CPU utilization (percentage): $ kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80
	- This command creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%

Horizontal Pod Autoscaling (HPA)

^^^^
Kubernetes makes a distinction between horizontal scaling, which involves creating additional replicas of a Pod, and vertical scaling,
which involves increasing the resources required for a particular Pod (e.g., increasing the CPU required for the Pod)

Web server like nginx, you may want to scale due to CPU usage. For an in-memory cache, you may want to scale with memory consumption. In some
cases you may want to scale in response to custom application metrics, hence HPA

!NOTE----NOTE----NOTE!

HPA requires the presence of the heapster Pod on your cluster. heapster keeps track of metrics and provides an API for consuming metrics HPA uses when making scaling
decisions. Most installations of Kubernetes include heapster by default. You can validate its presence by listing the Pods in the kube-system namespace:
	- $ kubectl get pods --namespace=kube-system

You should see a Pod named heapster somewhere in that list. If you do not see it,
autoscaling will not work correctly.

!NOTE----NOTE----NOTE!

*****************************
Chapter 9 DaemonSets (DaemonSets put a pod on each node)

By default a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of
labels.

Creating DaemonSets

Example 9-1. fluentd.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:v0.14.10
        resources:
          limits:
            memory: 200Mi
        requests:
          cpu: 100m
          memory: 200Mi
      volumeMounts:
      - name: varlog
        mountPath: /var/log
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers

*****************************
Chapter 13

StatefulSets
Automating MongoDB Cluster Creation
Manually replicated mongo db with stateful sets

*****************************

Chapter 14 Deploying Real-World Apps

Three examples used in this chapter:

Parse, an open source API server for mobile applications
	- replicate MongoDB using Kube StatefulSets
	- 1st, clone the Parse repo: git clone https://github.com/ParsePlatform/parse-server

Ghost, a blogging and content management platform

Redis, a lightweight, performant key/value store
	- consist of two programs: redis-server and redis-sentinel
		- redis-servers: slave (replica RO for LB) and master (RW)
		- redis-sentinel: health check and failovers

	- 3 config files are needed: master, slave and sentinel
	- 2 bash files: init.sh (to determine master/slave config to use on which server) and sentinel.sh for sentinel 
	- redis service needs to be created w/yaml file (w/o a cluster IP address)


Stateful vs Stateless


Deploying Parse server need three environment variables:
APPLICATION_ID
An identifier for authorizing your application

MASTER_KEY
An identifier that authorizes the master (root) user

DATABASE_URI
The URI for your MongoDB cluster
















							Misc Notes
				
*The kubernetes service is automatically created for you so that you can find and talk to the Kubernetes API from within the app.*
				
Naming and DNS is done using "kind: Service" yaml


YAML for exposing Kube service:

apiVersion: v1
kind: Service
metadata:
  name: parse-server
  namespace: default
spec:
  ports:
    - port: 1337
      protocol: TCP
      targetPort: 1337
  selector:
    run: parse-server




Example 14-11. redis.yaml

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: redis
spec:
  replicas: 3
  serviceName: redis
  template:
    metadata:
      labels:
        app: redis
  spec:
    containers:
    - command: [sh, -c, source /redis-config/init.sh ]
      image: redis:3.2.7-alpine
      name: redis
      ports:
      - containerPort: 6379
        name: redis
      volumeMounts:
      - mountPath: /redis-config
        name: config
      - mountPath: /redis-data
       name: data
    - command: [sh, -c, source /redis-config/sentinel.sh]
      image: redis:3.2.7-alpine
      name: sentinel
      volumeMounts:
      - mountPath: /redis-config
        name: config
      volumes:
      - configMap:
        defaultMode: 420
        name: redis-config
      name: config
    - emptyDir:
      name: data

^^NOTE^^
You can also note that there are two volumes defined in the yaml. One is the volume
that uses our ConfigMap to configure the two Redis applications, and the other is a
simple emptyDir volume that is mapped into the Redis server container to hold the
application data so that it survives a container restar
